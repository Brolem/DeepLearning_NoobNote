{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数的更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_mini_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     x_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[43mget_mini_batch\u001b[49m()\n\u001b[0;32m     10\u001b[0m     grads \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mgradient(x_batch, y_batch)\n\u001b[0;32m     11\u001b[0m     params \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mparams\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_mini_batch' is not defined"
     ]
    }
   ],
   "source": [
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "#深度学习框架实现了各种最优化方法，可以直接使用\n",
    "optimizer = SGD()\n",
    "\n",
    "for i in range(10000):\n",
    "    ...\n",
    "    x_batch, y_batch = get_mini_batch()\n",
    "    grads = network.gradient(x_batch, y_batch)\n",
    "    params = network.params\n",
    "    optimizer.update(parmas, grads) \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 多种最优化方法\n",
    "\n",
    "Momentum、Adagrad、Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "         \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = NOne\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key]*grads[key]\n",
    "            params[key] -= self.lr*grads[key]/(np.sqrt(self.h[key])+1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 权重初始值非常重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "优点\n",
    "1.可以使学习快速进行\n",
    "2.不那么依赖初始值\n",
    "3.抑制过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则化\n",
    "\n",
    "过拟合原因：\n",
    "1.模型拥有大量参数、表现力强\n",
    "2.训练数据少\n",
    "\n",
    "抑制过拟合的方法：\n",
    "1.权值衰减（L2正则化）\n",
    "2.Dropout正则化\n",
    "3.数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 超参数的验证\n",
    "\n",
    "不用使用测试数据评估超参数的性能，必须使用超参数专用的确认数据，一般称为验证数据。\n",
    "\n",
    "训练数据用于参数的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用测试数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从训练数据中事先分割20%作为验证数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "#打乱训练数据\n",
    "x_train, y_train = shuffle_dataset(x_train, t_train)\n",
    "\n",
    "#分割验证数据\n",
    "validation_rate = 0.2\n",
    "validatuion_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validatuion_num]\n",
    "t_val = t_train[:validatuion_num]\n",
    "\n",
    "x_train = x_train[validatuion_num:] \n",
    "t_train = t_train[validatuion_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 超参数的最优化\n",
    "\n",
    "逐渐缩小超参数的好值的存在范围（实践方法）\n",
    "\n",
    "贝叶斯最优化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
