{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整体结构\n",
    "\n",
    "Affine-ReLU连接被替换成了Convolution-ReLU-Pooling连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 卷积层和池化层的实现\n",
    "\n",
    "im2col函数的实现\n",
    "im2col(input_data, filter_h, filter_w, stride=1, pad=0)\n",
    "input_data: 输入数据，形状为(N, C, H, W)\n",
    "filter_h: 滤波器的高\n",
    "filter_w: 滤波器的宽\n",
    "stride: 步长，默认为1\n",
    "pad: 填充，默认为0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys,os \n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "coll = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(coll.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "coll = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(coll.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int((H + 2*self.pad - FH) / self.stride) + 1\n",
    "        out_w = int((W + 2*self.pad - FW) / self.stride) + 1\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T #滤波器展开\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int((H + 2 * self.pad - self.pool_h) / self.stride + 1)\n",
    "        out_w = int((W + 2 * self.pad - self.pool_w) / self.stride + 1)\n",
    "        \n",
    "        #展开（1）\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        #最大值（2）\n",
    "        out = np.max(col, axis=1)\n",
    "        #转换（3）\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN的实现\n",
    "\n",
    "网络构成为Convolution - ReLU -Pooling - Affine - ReLU - Affine - Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0,'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        #forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        #设定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299347510078876\n",
      "=== epoch:1, train acc:0.171, test acc:0.159 ===\n",
      "train loss:2.2968792167063437\n",
      "train loss:2.2888812859721814\n",
      "train loss:2.2862953406263924\n",
      "train loss:2.278695006010523\n",
      "train loss:2.269253139432814\n",
      "train loss:2.247427024402728\n",
      "train loss:2.232706783286117\n",
      "train loss:2.2057442915827936\n",
      "train loss:2.1809729131811064\n",
      "train loss:2.1351386514620496\n",
      "train loss:2.103354197898726\n",
      "train loss:2.05508002209125\n",
      "train loss:2.025501997260418\n",
      "train loss:1.9569845894440454\n",
      "train loss:1.8808196942919542\n",
      "train loss:1.8019317726861181\n",
      "train loss:1.73509225252134\n",
      "train loss:1.68022795551911\n",
      "train loss:1.532165730393816\n",
      "train loss:1.4006710511922664\n",
      "train loss:1.3435962233879264\n",
      "train loss:1.2920070441104208\n",
      "train loss:1.2169794784371302\n",
      "train loss:1.2235227879648347\n",
      "train loss:1.0090386444566806\n",
      "train loss:0.9979721067732406\n",
      "train loss:0.9900642848577728\n",
      "train loss:0.9063232773520898\n",
      "train loss:0.7801226715230202\n",
      "train loss:0.9107348673232049\n",
      "train loss:0.8387065492188346\n",
      "train loss:0.6686467901427293\n",
      "train loss:0.7483182529073029\n",
      "train loss:0.6792731588311723\n",
      "train loss:0.7028379409946832\n",
      "train loss:0.5701884162500375\n",
      "train loss:0.7263578720716317\n",
      "train loss:0.6120677594753883\n",
      "train loss:0.6686115573952026\n",
      "train loss:0.6720553182909995\n",
      "train loss:0.5530439772987784\n",
      "train loss:0.783709845452821\n",
      "train loss:0.5901215310789937\n",
      "train loss:0.9074294609483375\n",
      "train loss:0.5903753773727634\n",
      "train loss:0.604195576837535\n",
      "train loss:0.5550308157677416\n",
      "train loss:0.47023869279346653\n",
      "train loss:0.3777289609102495\n",
      "train loss:0.434480751821813\n",
      "=== epoch:2, train acc:0.808, test acc:0.795 ===\n",
      "train loss:0.6012557995691722\n",
      "train loss:0.5748700038983826\n",
      "train loss:0.47148499787256476\n",
      "train loss:0.39570692084500875\n",
      "train loss:0.4436493237094522\n",
      "train loss:0.5426934694409183\n",
      "train loss:0.43853493369084684\n",
      "train loss:0.4790962236817002\n",
      "train loss:0.4980073612891482\n",
      "train loss:0.5341066062083294\n",
      "train loss:0.47290453025995904\n",
      "train loss:0.37045920740087895\n",
      "train loss:0.43116840004991047\n",
      "train loss:0.4335819708390021\n",
      "train loss:0.3535237388044474\n",
      "train loss:0.4873540359897018\n",
      "train loss:0.5493090419874553\n",
      "train loss:0.3332904639659042\n",
      "train loss:0.528485979562962\n",
      "train loss:0.2904643250672947\n",
      "train loss:0.40381225020352296\n",
      "train loss:0.5667574894036286\n",
      "train loss:0.3620512358066336\n",
      "train loss:0.3008779500302877\n",
      "train loss:0.37050516979140885\n",
      "train loss:0.26769812786955965\n",
      "train loss:0.388365755205542\n",
      "train loss:0.2971589467821748\n",
      "train loss:0.5307352438214955\n",
      "train loss:0.41039127984036894\n",
      "train loss:0.3813167054091034\n",
      "train loss:0.4119942984506041\n",
      "train loss:0.2970041394789628\n",
      "train loss:0.3077459910844385\n",
      "train loss:0.3794113800622572\n",
      "train loss:0.49538762449007856\n",
      "train loss:0.3549980995785837\n",
      "train loss:0.44203229774931896\n",
      "train loss:0.2620539597935586\n",
      "train loss:0.28114434974202207\n",
      "train loss:0.39450820584357355\n",
      "train loss:0.4557035332434941\n",
      "train loss:0.26528914428144357\n",
      "train loss:0.23903050049173796\n",
      "train loss:0.24155961255303932\n",
      "train loss:0.2684473946485839\n",
      "train loss:0.3955508668224382\n",
      "train loss:0.17673519804896962\n",
      "train loss:0.2524176032850616\n",
      "train loss:0.49501550846563674\n",
      "=== epoch:3, train acc:0.878, test acc:0.868 ===\n",
      "train loss:0.3589705685321437\n",
      "train loss:0.23943497757889015\n",
      "train loss:0.31065783664856506\n",
      "train loss:0.3296218372814541\n",
      "train loss:0.22825786220051547\n",
      "train loss:0.3359234410241531\n",
      "train loss:0.2486176890485151\n",
      "train loss:0.3977488535439439\n",
      "train loss:0.20526607153506282\n",
      "train loss:0.4848952209562366\n",
      "train loss:0.2579918980405802\n",
      "train loss:0.3008618431953959\n",
      "train loss:0.2590324251463754\n",
      "train loss:0.23566891283280186\n",
      "train loss:0.23435101294748037\n",
      "train loss:0.44940322665172094\n",
      "train loss:0.2840252114705056\n",
      "train loss:0.1910996134810417\n",
      "train loss:0.29023933359419823\n",
      "train loss:0.21810935535929807\n",
      "train loss:0.27376751217597795\n",
      "train loss:0.18894206214169956\n",
      "train loss:0.21718294761311838\n",
      "train loss:0.24229850737414665\n",
      "train loss:0.22071719534695305\n",
      "train loss:0.2702005230269218\n",
      "train loss:0.21538727126906598\n",
      "train loss:0.11349248583645152\n",
      "train loss:0.3703787934221316\n",
      "train loss:0.24350242495388735\n",
      "train loss:0.28562014498040794\n",
      "train loss:0.278199341675255\n",
      "train loss:0.24841678464245015\n",
      "train loss:0.25475878689441034\n",
      "train loss:0.3070460299296251\n",
      "train loss:0.2650086530245185\n",
      "train loss:0.2346382741389531\n",
      "train loss:0.21810376595337128\n",
      "train loss:0.225898023016675\n",
      "train loss:0.33037639389343537\n",
      "train loss:0.34448977320380225\n",
      "train loss:0.1947501235106771\n",
      "train loss:0.21332616683894656\n",
      "train loss:0.2590952755835263\n",
      "train loss:0.2572821955949535\n",
      "train loss:0.23986862675084686\n",
      "train loss:0.25101203802111266\n",
      "train loss:0.32662456909229964\n",
      "train loss:0.28010971276269514\n",
      "train loss:0.3940019304988441\n",
      "=== epoch:4, train acc:0.9, test acc:0.895 ===\n",
      "train loss:0.2867469882280123\n",
      "train loss:0.28805387258453835\n",
      "train loss:0.2373503384942541\n",
      "train loss:0.24590405460051187\n",
      "train loss:0.16664553052730308\n",
      "train loss:0.27352086613444465\n",
      "train loss:0.24612089240702514\n",
      "train loss:0.2591131647870402\n",
      "train loss:0.30645739928811233\n",
      "train loss:0.3370970644120458\n",
      "train loss:0.2816898857881312\n",
      "train loss:0.24851881645900942\n",
      "train loss:0.2474215664346575\n",
      "train loss:0.2341378960574008\n",
      "train loss:0.2699758941277131\n",
      "train loss:0.36125799369134315\n",
      "train loss:0.25219013447171895\n",
      "train loss:0.5386188892049583\n",
      "train loss:0.2719394506439529\n",
      "train loss:0.20735135646124614\n",
      "train loss:0.36249837016018693\n",
      "train loss:0.30030114609051006\n",
      "train loss:0.1909599899131581\n",
      "train loss:0.23344783289028176\n",
      "train loss:0.25147831056487696\n",
      "train loss:0.16717624009213958\n",
      "train loss:0.29972558791343884\n",
      "train loss:0.18743214770790043\n",
      "train loss:0.1347901441776669\n",
      "train loss:0.15288450677884213\n",
      "train loss:0.1671482834326221\n",
      "train loss:0.2592485713828083\n",
      "train loss:0.19642391184492364\n",
      "train loss:0.18599738836600055\n",
      "train loss:0.15467854136622458\n",
      "train loss:0.3990167483105712\n",
      "train loss:0.12332609416531434\n",
      "train loss:0.22583257890269895\n",
      "train loss:0.2361922764283005\n",
      "train loss:0.31659296375863677\n",
      "train loss:0.16024696446888542\n",
      "train loss:0.18959297082254373\n",
      "train loss:0.152956925806161\n",
      "train loss:0.20232553007557857\n",
      "train loss:0.21821517663067755\n",
      "train loss:0.13806566655101085\n",
      "train loss:0.32078442457005557\n",
      "train loss:0.32201455393758116\n",
      "train loss:0.20295266205488804\n",
      "train loss:0.43298033917921813\n",
      "=== epoch:5, train acc:0.913, test acc:0.889 ===\n",
      "train loss:0.2516989058741438\n",
      "train loss:0.20380094950748107\n",
      "train loss:0.270993373251791\n",
      "train loss:0.35386442131527424\n",
      "train loss:0.21631861477672903\n",
      "train loss:0.23027825123520093\n",
      "train loss:0.18822537451061538\n",
      "train loss:0.25395305466022544\n",
      "train loss:0.32565511301851996\n",
      "train loss:0.27067328275899544\n",
      "train loss:0.30210004741487834\n",
      "train loss:0.1964323871325393\n",
      "train loss:0.17794564699427035\n",
      "train loss:0.1843431544621388\n",
      "train loss:0.24351325969756357\n",
      "train loss:0.30411692007589747\n",
      "train loss:0.17363931202950758\n",
      "train loss:0.15993643702721858\n",
      "train loss:0.1794205563231861\n",
      "train loss:0.14632451241488215\n",
      "train loss:0.22444832056301053\n",
      "train loss:0.20423687101641583\n",
      "train loss:0.28212635361956395\n",
      "train loss:0.17380763557657125\n",
      "train loss:0.12835742595707333\n",
      "train loss:0.2375466139386573\n",
      "train loss:0.25091768783966223\n",
      "train loss:0.2574014154573522\n",
      "train loss:0.1643575156824019\n",
      "train loss:0.15288844128215354\n",
      "train loss:0.09329797408245456\n",
      "train loss:0.2791657996481591\n",
      "train loss:0.14248838763265217\n",
      "train loss:0.3212847568254284\n",
      "train loss:0.1851275043863817\n",
      "train loss:0.19812427465194019\n",
      "train loss:0.13266789553047065\n",
      "train loss:0.16785189575522297\n",
      "train loss:0.15904039727547697\n",
      "train loss:0.19399428961711374\n",
      "train loss:0.16118797809195365\n",
      "train loss:0.15984237379167468\n",
      "train loss:0.1405577677162513\n",
      "train loss:0.1961662148784259\n",
      "train loss:0.38229916576677747\n",
      "train loss:0.2714069750374241\n",
      "train loss:0.20627254123138988\n",
      "train loss:0.2571354442752494\n",
      "train loss:0.11504934821261617\n",
      "train loss:0.11587758369767108\n",
      "=== epoch:6, train acc:0.934, test acc:0.909 ===\n",
      "train loss:0.13413393600122214\n",
      "train loss:0.15812105384168457\n",
      "train loss:0.11248541213133686\n",
      "train loss:0.3258823419726219\n",
      "train loss:0.14900394466059239\n",
      "train loss:0.13150675998774314\n",
      "train loss:0.18618438484435423\n",
      "train loss:0.142016033930903\n",
      "train loss:0.19551656733122713\n",
      "train loss:0.24804114362853177\n",
      "train loss:0.1423730945865294\n",
      "train loss:0.16479597701228954\n",
      "train loss:0.16917578748945744\n",
      "train loss:0.07207248130156263\n",
      "train loss:0.1057535712383483\n",
      "train loss:0.21303524105228813\n",
      "train loss:0.07700568743495145\n",
      "train loss:0.11232743643990149\n",
      "train loss:0.2041841610655954\n",
      "train loss:0.05425052420055598\n",
      "train loss:0.28197662019820746\n",
      "train loss:0.25107858038294834\n",
      "train loss:0.09680322459883282\n",
      "train loss:0.21950170530047822\n",
      "train loss:0.22478325946798455\n",
      "train loss:0.16474845113380077\n",
      "train loss:0.26198598850541194\n",
      "train loss:0.07795266294008027\n",
      "train loss:0.18787540703765743\n",
      "train loss:0.21576549851925558\n",
      "train loss:0.23902799880147185\n",
      "train loss:0.16926815214108137\n",
      "train loss:0.15717543771119888\n",
      "train loss:0.15821074523942616\n",
      "train loss:0.15974472147757893\n",
      "train loss:0.19259795693704565\n",
      "train loss:0.0912212585191577\n",
      "train loss:0.09266917153318847\n",
      "train loss:0.3202877670001949\n",
      "train loss:0.10286248278796851\n",
      "train loss:0.15005038207758914\n",
      "train loss:0.15801177157439772\n",
      "train loss:0.07743106406517387\n",
      "train loss:0.08263096072875036\n",
      "train loss:0.22303854191677375\n",
      "train loss:0.2563223329766722\n",
      "train loss:0.10172450664718947\n",
      "train loss:0.1534039763564905\n",
      "train loss:0.14768642049137984\n",
      "train loss:0.19751172980056997\n",
      "=== epoch:7, train acc:0.946, test acc:0.924 ===\n",
      "train loss:0.21625983452471179\n",
      "train loss:0.14502376916562704\n",
      "train loss:0.17297968094645036\n",
      "train loss:0.138378010583678\n",
      "train loss:0.057581121449150625\n",
      "train loss:0.19595338063422144\n",
      "train loss:0.15785437998300852\n",
      "train loss:0.15618202694331015\n",
      "train loss:0.18259165215306147\n",
      "train loss:0.1550609151572721\n",
      "train loss:0.09505608540409749\n",
      "train loss:0.14194360013776777\n",
      "train loss:0.27085104222285245\n",
      "train loss:0.1392439723758324\n",
      "train loss:0.08335470850534843\n",
      "train loss:0.149486708368304\n",
      "train loss:0.2051602819254019\n",
      "train loss:0.1326111945616936\n",
      "train loss:0.058928860153350984\n",
      "train loss:0.1237390360659087\n",
      "train loss:0.09056616962810818\n",
      "train loss:0.11327166644123073\n",
      "train loss:0.15004820880994643\n",
      "train loss:0.23017908986370317\n",
      "train loss:0.40322062242685447\n",
      "train loss:0.24475148109291786\n",
      "train loss:0.06689661934089954\n",
      "train loss:0.1251722626487775\n",
      "train loss:0.1771803644535795\n",
      "train loss:0.3129330799612456\n",
      "train loss:0.10701768156938657\n",
      "train loss:0.12221099589148823\n",
      "train loss:0.08780833888829159\n",
      "train loss:0.15732486064034043\n",
      "train loss:0.0936262959997412\n",
      "train loss:0.15551547772834298\n",
      "train loss:0.13408350956332235\n",
      "train loss:0.15843283890515095\n",
      "train loss:0.10983065893314212\n",
      "train loss:0.09815480832802584\n",
      "train loss:0.0542943959867831\n",
      "train loss:0.0835293275846149\n",
      "train loss:0.09911007823949484\n",
      "train loss:0.13339247708151147\n",
      "train loss:0.12169540822070724\n",
      "train loss:0.13534271924934146\n",
      "train loss:0.1808881054847993\n",
      "train loss:0.13162501843034982\n",
      "train loss:0.0871770507131004\n",
      "train loss:0.11224464053973505\n",
      "=== epoch:8, train acc:0.942, test acc:0.93 ===\n",
      "train loss:0.10708419475507183\n",
      "train loss:0.0717036369436069\n",
      "train loss:0.0966877041238598\n",
      "train loss:0.08380502788372483\n",
      "train loss:0.1696707801434699\n",
      "train loss:0.15211865155695775\n",
      "train loss:0.19630511929452607\n",
      "train loss:0.09993961154674931\n",
      "train loss:0.09594612639180009\n",
      "train loss:0.08590255157254191\n",
      "train loss:0.10712972125188598\n",
      "train loss:0.20611254669094645\n",
      "train loss:0.11714788634269063\n",
      "train loss:0.13174972206559185\n",
      "train loss:0.2406782987360346\n",
      "train loss:0.05954708627716526\n",
      "train loss:0.10571619680061835\n",
      "train loss:0.10063336395138042\n",
      "train loss:0.11740850302436223\n",
      "train loss:0.17193839459121293\n",
      "train loss:0.16471487048525119\n",
      "train loss:0.10530816239111374\n",
      "train loss:0.09159900776472481\n",
      "train loss:0.16126205591634854\n",
      "train loss:0.04713580437481259\n",
      "train loss:0.1345345693459486\n",
      "train loss:0.1847314360521403\n",
      "train loss:0.11323140206525849\n",
      "train loss:0.14881210762268737\n",
      "train loss:0.0867623464225271\n",
      "train loss:0.08451055244020096\n",
      "train loss:0.09797047337487742\n",
      "train loss:0.09227802321603008\n",
      "train loss:0.154273419063865\n",
      "train loss:0.03737590171725493\n",
      "train loss:0.16096516166472646\n",
      "train loss:0.08641892562440427\n",
      "train loss:0.22783141875450078\n",
      "train loss:0.10836685648451488\n",
      "train loss:0.10998079652371548\n",
      "train loss:0.07592348260918799\n",
      "train loss:0.09815112432228994\n",
      "train loss:0.14366422704441684\n",
      "train loss:0.08860102024594604\n",
      "train loss:0.07679782020235092\n",
      "train loss:0.11512266912247555\n",
      "train loss:0.14079291205760644\n",
      "train loss:0.1234484687667724\n",
      "train loss:0.14913345556875654\n",
      "train loss:0.07892981723817093\n",
      "=== epoch:9, train acc:0.957, test acc:0.936 ===\n",
      "train loss:0.10371546873637555\n",
      "train loss:0.14246896768224798\n",
      "train loss:0.15693271132218656\n",
      "train loss:0.030956949865243787\n",
      "train loss:0.07633358319481606\n",
      "train loss:0.12744251047795085\n",
      "train loss:0.16778966235143813\n",
      "train loss:0.08506548647498972\n",
      "train loss:0.10324398747496945\n",
      "train loss:0.06385752037925099\n",
      "train loss:0.10669993073290662\n",
      "train loss:0.11645505832291876\n",
      "train loss:0.10043210155445897\n",
      "train loss:0.09654473640194769\n",
      "train loss:0.0651065486593741\n",
      "train loss:0.05508926423237062\n",
      "train loss:0.08659214605735795\n",
      "train loss:0.08509655917085929\n",
      "train loss:0.10226814456381379\n",
      "train loss:0.0802497742255249\n",
      "train loss:0.07526168574394404\n",
      "train loss:0.1604041694928123\n",
      "train loss:0.049316040196211\n",
      "train loss:0.0448072357486127\n",
      "train loss:0.16938214164507218\n",
      "train loss:0.06477811711824004\n",
      "train loss:0.09450477901391809\n",
      "train loss:0.18586342128825034\n",
      "train loss:0.05613717139911565\n",
      "train loss:0.07914715414802727\n",
      "train loss:0.10063217584544427\n",
      "train loss:0.07810364473562448\n",
      "train loss:0.07821171914407998\n",
      "train loss:0.07028951236028608\n",
      "train loss:0.08912804932924188\n",
      "train loss:0.14652751496592706\n",
      "train loss:0.07926009390913649\n",
      "train loss:0.049384689517608917\n",
      "train loss:0.09293658681148452\n",
      "train loss:0.07491085517131407\n",
      "train loss:0.15180417761442178\n",
      "train loss:0.08078879611515478\n",
      "train loss:0.09931301009357957\n",
      "train loss:0.10814859708054853\n",
      "train loss:0.06730540569987656\n",
      "train loss:0.09980213447189681\n",
      "train loss:0.19426131954415035\n",
      "train loss:0.06455349006831514\n",
      "train loss:0.11822306456574419\n",
      "train loss:0.07246779103025573\n",
      "=== epoch:10, train acc:0.965, test acc:0.95 ===\n",
      "train loss:0.04280300824651159\n",
      "train loss:0.03740208873155884\n",
      "train loss:0.09507215377729915\n",
      "train loss:0.12588017048384736\n",
      "train loss:0.14321119556282133\n",
      "train loss:0.08723161919806773\n",
      "train loss:0.0863570605230815\n",
      "train loss:0.04316889056838641\n",
      "train loss:0.1787390442879661\n",
      "train loss:0.03425781086859225\n",
      "train loss:0.0385580053187609\n",
      "train loss:0.12253516928490099\n",
      "train loss:0.04509866292811035\n",
      "train loss:0.08969702687056386\n",
      "train loss:0.12102140615240449\n",
      "train loss:0.03213485069730012\n",
      "train loss:0.12025173257787009\n",
      "train loss:0.04713215076305723\n",
      "train loss:0.09543206074218082\n",
      "train loss:0.13574846213036465\n",
      "train loss:0.08111517130796239\n",
      "train loss:0.11458111179734885\n",
      "train loss:0.05851028301907719\n",
      "train loss:0.05531668335850434\n",
      "train loss:0.1460272092438555\n",
      "train loss:0.0994350820066136\n",
      "train loss:0.10061755954740415\n",
      "train loss:0.1699204363516625\n",
      "train loss:0.05351704316801839\n",
      "train loss:0.05856302678911038\n",
      "train loss:0.044602200245617896\n",
      "train loss:0.0927233221459174\n",
      "train loss:0.04287208890782005\n",
      "train loss:0.09775217426056504\n",
      "train loss:0.06045726278824634\n",
      "train loss:0.07397691479769569\n",
      "train loss:0.0895085456601659\n",
      "train loss:0.16461467345893502\n",
      "train loss:0.17105284722251393\n",
      "train loss:0.06506635529623554\n",
      "train loss:0.11643173999531457\n",
      "train loss:0.0850466317132788\n",
      "train loss:0.09951194089282507\n",
      "train loss:0.061573674270027606\n",
      "train loss:0.0870485118537502\n",
      "train loss:0.09270534195007706\n",
      "train loss:0.09493026487684902\n",
      "train loss:0.03845592796172598\n",
      "train loss:0.05680638541443871\n",
      "train loss:0.07081126324059134\n",
      "=== epoch:11, train acc:0.966, test acc:0.945 ===\n",
      "train loss:0.09512316496178437\n",
      "train loss:0.05715008364797944\n",
      "train loss:0.06101242694013608\n",
      "train loss:0.0904995677011153\n",
      "train loss:0.1271131363222421\n",
      "train loss:0.04762545072377102\n",
      "train loss:0.041154498436613667\n",
      "train loss:0.0412573209651853\n",
      "train loss:0.05264960506629688\n",
      "train loss:0.04221875733010428\n",
      "train loss:0.0513783194056161\n",
      "train loss:0.08748996050684615\n",
      "train loss:0.11136801663679613\n",
      "train loss:0.0658097607968693\n",
      "train loss:0.06452166513894604\n",
      "train loss:0.04298444545436077\n",
      "train loss:0.18051733560010302\n",
      "train loss:0.07695814568094739\n",
      "train loss:0.07273643851519113\n",
      "train loss:0.14534233128789553\n",
      "train loss:0.0723006973450871\n",
      "train loss:0.07399761749263367\n",
      "train loss:0.08772420071050456\n",
      "train loss:0.0758906608702017\n",
      "train loss:0.06650569881041092\n",
      "train loss:0.0953151779968943\n",
      "train loss:0.029994963726306327\n",
      "train loss:0.053616321597646024\n",
      "train loss:0.05034198809288581\n",
      "train loss:0.18007070302932512\n",
      "train loss:0.14641751569923914\n",
      "train loss:0.05995711290698062\n",
      "train loss:0.11119165870292518\n",
      "train loss:0.08847975059671823\n",
      "train loss:0.06481665060208455\n",
      "train loss:0.06405244078843327\n",
      "train loss:0.07768195711106336\n",
      "train loss:0.03086405302623296\n",
      "train loss:0.043857242132498524\n",
      "train loss:0.0824151547816142\n",
      "train loss:0.11622179285015838\n",
      "train loss:0.04843913810350291\n",
      "train loss:0.05456606937971395\n",
      "train loss:0.09005296455487272\n",
      "train loss:0.07731746178402356\n",
      "train loss:0.059771228673693576\n",
      "train loss:0.023625396773364112\n",
      "train loss:0.06806470530700975\n",
      "train loss:0.07988361750691905\n",
      "train loss:0.057219728896072394\n",
      "=== epoch:12, train acc:0.965, test acc:0.951 ===\n",
      "train loss:0.11305387393517799\n",
      "train loss:0.09990555475902291\n",
      "train loss:0.11473813736606109\n",
      "train loss:0.03624966985862719\n",
      "train loss:0.09087154648666257\n",
      "train loss:0.08680674558387222\n",
      "train loss:0.05865118256204604\n",
      "train loss:0.03408614510712911\n",
      "train loss:0.12025992307479932\n",
      "train loss:0.020344518652864935\n",
      "train loss:0.06087002090997553\n",
      "train loss:0.050991830162715156\n",
      "train loss:0.04894660471686414\n",
      "train loss:0.03911241560922191\n",
      "train loss:0.04390776770466411\n",
      "train loss:0.09947589643784863\n",
      "train loss:0.02212388489025731\n",
      "train loss:0.056727341998351964\n",
      "train loss:0.12472382021203807\n",
      "train loss:0.06850310421510439\n",
      "train loss:0.08657882067794961\n",
      "train loss:0.11982815905031412\n",
      "train loss:0.045993858703613597\n",
      "train loss:0.0339817512866233\n",
      "train loss:0.07444216161375647\n",
      "train loss:0.06366489080714678\n",
      "train loss:0.03161065249388455\n",
      "train loss:0.0552329445897104\n",
      "train loss:0.0488803303043515\n",
      "train loss:0.046558689601343926\n",
      "train loss:0.09327262509950536\n",
      "train loss:0.026255207474196216\n",
      "train loss:0.05486497190420607\n",
      "train loss:0.07656637709695603\n",
      "train loss:0.033208500967143915\n",
      "train loss:0.10338528264022118\n",
      "train loss:0.039949008568061044\n",
      "train loss:0.07603554439882605\n",
      "train loss:0.02747689980769918\n",
      "train loss:0.09140682116448741\n",
      "train loss:0.05665940053331031\n",
      "train loss:0.02672425390962684\n",
      "train loss:0.02516315077557096\n",
      "train loss:0.07296486230905308\n",
      "train loss:0.07814427850790426\n",
      "train loss:0.07352115031691511\n",
      "train loss:0.02242928301728599\n",
      "train loss:0.0674450146982954\n",
      "train loss:0.029114444181314984\n",
      "train loss:0.036145022536069014\n",
      "=== epoch:13, train acc:0.977, test acc:0.958 ===\n",
      "train loss:0.055550081416094244\n",
      "train loss:0.11343028776061236\n",
      "train loss:0.055572765390867127\n",
      "train loss:0.05261723428282243\n",
      "train loss:0.024708532835319326\n",
      "train loss:0.09647920172507972\n",
      "train loss:0.05418667675646964\n",
      "train loss:0.113245466667792\n",
      "train loss:0.10185976564827275\n",
      "train loss:0.02369858211261346\n",
      "train loss:0.021748264039180717\n",
      "train loss:0.03441911164877571\n",
      "train loss:0.043756876355871104\n",
      "train loss:0.040990604323914256\n",
      "train loss:0.09018151137175832\n",
      "train loss:0.04982159573261882\n",
      "train loss:0.09404333777121289\n",
      "train loss:0.05196302883970679\n",
      "train loss:0.047174080284623846\n",
      "train loss:0.0978052747978658\n",
      "train loss:0.027799821293740513\n",
      "train loss:0.09201889849316533\n",
      "train loss:0.032973454099949555\n",
      "train loss:0.0818911048552983\n",
      "train loss:0.05816746207895657\n",
      "train loss:0.07360390903525756\n",
      "train loss:0.051110380825335595\n",
      "train loss:0.016861470756090655\n",
      "train loss:0.024079751167579194\n",
      "train loss:0.08317455196216546\n",
      "train loss:0.05651316794769822\n",
      "train loss:0.07107372955005785\n",
      "train loss:0.025144832547681135\n",
      "train loss:0.07155512133214953\n",
      "train loss:0.05866609864144516\n",
      "train loss:0.021161716276911578\n",
      "train loss:0.0709989468629354\n",
      "train loss:0.03275358421718546\n",
      "train loss:0.03486298336004276\n",
      "train loss:0.03365095142423983\n",
      "train loss:0.05584673428254525\n",
      "train loss:0.06735001655962945\n",
      "train loss:0.02484439327613508\n",
      "train loss:0.026056625186088395\n",
      "train loss:0.041765780514563\n",
      "train loss:0.05366028905600933\n",
      "train loss:0.05801577943014538\n",
      "train loss:0.025860444892756487\n",
      "train loss:0.019870556047980114\n",
      "train loss:0.06038940858456409\n",
      "=== epoch:14, train acc:0.974, test acc:0.956 ===\n",
      "train loss:0.037482148612726744\n",
      "train loss:0.03783720085428602\n",
      "train loss:0.01718518034054954\n",
      "train loss:0.027243702359688694\n",
      "train loss:0.02197202654954576\n",
      "train loss:0.09079343886411267\n",
      "train loss:0.047170386255936086\n",
      "train loss:0.04406589981803331\n",
      "train loss:0.027549054735234845\n",
      "train loss:0.026570414736724616\n",
      "train loss:0.05495541997889105\n",
      "train loss:0.05228082871786266\n",
      "train loss:0.015369086427781824\n",
      "train loss:0.06261327282111331\n",
      "train loss:0.059983234776520844\n",
      "train loss:0.04889451690146344\n",
      "train loss:0.03990349474776349\n",
      "train loss:0.0791796796036618\n",
      "train loss:0.11319120995755519\n",
      "train loss:0.03297394244542696\n",
      "train loss:0.04681136500343196\n",
      "train loss:0.05352294254077874\n",
      "train loss:0.014073647883725247\n",
      "train loss:0.0577078521665023\n",
      "train loss:0.0509811061602887\n",
      "train loss:0.08672645766784846\n",
      "train loss:0.03848801862730419\n",
      "train loss:0.036890790434595415\n",
      "train loss:0.05594545674455559\n",
      "train loss:0.03235226340247775\n",
      "train loss:0.02491822440379365\n",
      "train loss:0.05005171944029832\n",
      "train loss:0.0963487416850259\n",
      "train loss:0.0541011032171397\n",
      "train loss:0.09219785805775875\n",
      "train loss:0.0851602855845722\n",
      "train loss:0.0961573335447424\n",
      "train loss:0.023376219919118545\n",
      "train loss:0.03803612565234035\n",
      "train loss:0.0297860092810054\n",
      "train loss:0.08122442034367161\n",
      "train loss:0.008364107913765826\n",
      "train loss:0.02731100617504148\n",
      "train loss:0.020167943116166338\n",
      "train loss:0.06037896081264313\n",
      "train loss:0.0636471701589836\n",
      "train loss:0.023917471418518237\n",
      "train loss:0.04629487438193488\n",
      "train loss:0.03286916432571526\n",
      "train loss:0.08126429538291903\n",
      "=== epoch:15, train acc:0.986, test acc:0.958 ===\n",
      "train loss:0.05809975315581304\n",
      "train loss:0.02481967036841797\n",
      "train loss:0.040896829354851096\n",
      "train loss:0.06620005722676996\n",
      "train loss:0.03990208573080917\n",
      "train loss:0.03864259029080047\n",
      "train loss:0.03698605242520367\n",
      "train loss:0.05483624701739464\n",
      "train loss:0.06905765468402758\n",
      "train loss:0.04026535718554352\n",
      "train loss:0.0781020871406069\n",
      "train loss:0.09624361276851207\n",
      "train loss:0.0798010097112263\n",
      "train loss:0.07074845143139216\n",
      "train loss:0.03141547592430255\n",
      "train loss:0.0675560687947121\n",
      "train loss:0.03276528359308201\n",
      "train loss:0.031697128851360376\n",
      "train loss:0.057398270022785046\n",
      "train loss:0.01685029927048617\n",
      "train loss:0.07253651019631895\n",
      "train loss:0.04921456737290374\n",
      "train loss:0.0331141250009349\n",
      "train loss:0.0535908242832342\n",
      "train loss:0.03140269084244853\n",
      "train loss:0.07701402714688747\n",
      "train loss:0.09540559269583339\n",
      "train loss:0.0351139566601341\n",
      "train loss:0.04294035675627895\n",
      "train loss:0.029348355528444454\n",
      "train loss:0.03450904916169232\n",
      "train loss:0.01619134368711243\n",
      "train loss:0.10509014806892504\n",
      "train loss:0.033300540151754765\n",
      "train loss:0.060650420800627235\n",
      "train loss:0.05490426267870459\n",
      "train loss:0.031954206857799254\n",
      "train loss:0.0477235360828133\n",
      "train loss:0.03249123327245141\n",
      "train loss:0.07401477381808416\n",
      "train loss:0.014980424327742402\n",
      "train loss:0.03603846583223197\n",
      "train loss:0.06564285119524076\n",
      "train loss:0.02101903678748821\n",
      "train loss:0.03056163561649467\n",
      "train loss:0.028848926288050067\n",
      "train loss:0.01629923302459752\n",
      "train loss:0.023103183070268044\n",
      "train loss:0.018373142204269582\n",
      "train loss:0.05693795288207205\n",
      "=== epoch:16, train acc:0.988, test acc:0.956 ===\n",
      "train loss:0.019699147417278146\n",
      "train loss:0.03259118468291272\n",
      "train loss:0.012452590003852041\n",
      "train loss:0.02478402238979457\n",
      "train loss:0.046662421899382835\n",
      "train loss:0.04015942531207051\n",
      "train loss:0.02997888568239135\n",
      "train loss:0.015022721216646101\n",
      "train loss:0.02593203445946206\n",
      "train loss:0.023180509867339966\n",
      "train loss:0.0198372140256553\n",
      "train loss:0.03234810588515716\n",
      "train loss:0.0438056561542808\n",
      "train loss:0.02415938808964836\n",
      "train loss:0.03891508115877433\n",
      "train loss:0.028336280274405344\n",
      "train loss:0.026253773751940126\n",
      "train loss:0.021622167124468402\n",
      "train loss:0.031594071696759374\n",
      "train loss:0.033786927307199704\n",
      "train loss:0.05185231853604732\n",
      "train loss:0.019205206872236008\n",
      "train loss:0.04406827942927688\n",
      "train loss:0.04616029804818588\n",
      "train loss:0.029209406986162798\n",
      "train loss:0.08370679166983401\n",
      "train loss:0.013725904456623982\n",
      "train loss:0.008522062723231883\n",
      "train loss:0.04426583335795565\n",
      "train loss:0.030571033160434403\n",
      "train loss:0.021249084117423177\n",
      "train loss:0.057773757502455156\n",
      "train loss:0.014007622041995058\n",
      "train loss:0.022080059723100964\n",
      "train loss:0.011696420556709155\n",
      "train loss:0.04627738834641651\n",
      "train loss:0.09594806274123653\n",
      "train loss:0.03585145670693422\n",
      "train loss:0.03035916683348697\n",
      "train loss:0.013406315070958446\n",
      "train loss:0.03329698576422707\n",
      "train loss:0.01218102936943174\n",
      "train loss:0.06741313218472694\n",
      "train loss:0.024168047658346854\n",
      "train loss:0.02952506615983133\n",
      "train loss:0.015953812630156078\n",
      "train loss:0.04089225688162555\n",
      "train loss:0.0381778241089121\n",
      "train loss:0.0232997859875954\n",
      "train loss:0.018552450837788025\n",
      "=== epoch:17, train acc:0.992, test acc:0.962 ===\n",
      "train loss:0.025765277027927403\n",
      "train loss:0.03642134360474839\n",
      "train loss:0.0243988346156552\n",
      "train loss:0.06736873306049831\n",
      "train loss:0.034794645769273214\n",
      "train loss:0.01076487591322125\n",
      "train loss:0.021886284530838473\n",
      "train loss:0.0393310838588648\n",
      "train loss:0.10864083941780946\n",
      "train loss:0.0250287974071734\n",
      "train loss:0.07397417556434227\n",
      "train loss:0.02390866205359735\n",
      "train loss:0.02470071403593802\n",
      "train loss:0.022871759884399167\n",
      "train loss:0.04105698628141502\n",
      "train loss:0.022265265387922745\n",
      "train loss:0.01915393811789883\n",
      "train loss:0.03862812694989134\n",
      "train loss:0.014033228001401308\n",
      "train loss:0.03163142317604705\n",
      "train loss:0.018755106925215495\n",
      "train loss:0.02856647407302932\n",
      "train loss:0.015611813814104954\n",
      "train loss:0.007913933359009794\n",
      "train loss:0.008640962866244445\n",
      "train loss:0.015357234825131951\n",
      "train loss:0.03908570970123977\n",
      "train loss:0.011786100430385991\n",
      "train loss:0.01985176293919892\n",
      "train loss:0.03336710824407169\n",
      "train loss:0.017808648306527997\n",
      "train loss:0.011504967156083488\n",
      "train loss:0.017981777853177023\n",
      "train loss:0.023558190110226113\n",
      "train loss:0.03095570169971117\n",
      "train loss:0.021562845293105787\n",
      "train loss:0.036434873742415184\n",
      "train loss:0.01362667518368136\n",
      "train loss:0.021093649275077078\n",
      "train loss:0.010416338092407069\n",
      "train loss:0.015981362394665726\n",
      "train loss:0.02515633285530032\n",
      "train loss:0.0234037932974057\n",
      "train loss:0.044770183331716895\n",
      "train loss:0.022361110602385114\n",
      "train loss:0.009904290598613707\n",
      "train loss:0.022096920275685283\n",
      "train loss:0.04406131250949092\n",
      "train loss:0.015468578698121842\n",
      "train loss:0.019871402607178318\n",
      "=== epoch:18, train acc:0.99, test acc:0.963 ===\n",
      "train loss:0.06191947095485637\n",
      "train loss:0.021694315684074175\n",
      "train loss:0.042624271298527215\n",
      "train loss:0.034276096995678046\n",
      "train loss:0.02062651385195617\n",
      "train loss:0.0149670818364608\n",
      "train loss:0.056641322074973334\n",
      "train loss:0.01079640392381893\n",
      "train loss:0.016154514972901296\n",
      "train loss:0.011020600141762416\n",
      "train loss:0.02486054506971854\n",
      "train loss:0.02222898468388371\n",
      "train loss:0.00981479224438413\n",
      "train loss:0.017388317762484177\n",
      "train loss:0.006536776115886355\n",
      "train loss:0.03706916651591242\n",
      "train loss:0.00978776794751013\n",
      "train loss:0.016963522973743084\n",
      "train loss:0.029286867220397864\n",
      "train loss:0.045524490693375184\n",
      "train loss:0.018975143833254608\n",
      "train loss:0.010214305216008081\n",
      "train loss:0.040324242415175855\n",
      "train loss:0.011696226620962202\n",
      "train loss:0.017253810746608474\n",
      "train loss:0.015303087530265089\n",
      "train loss:0.02217661948449548\n",
      "train loss:0.05675188133526588\n",
      "train loss:0.02711956471153384\n",
      "train loss:0.03086998030779713\n",
      "train loss:0.01986743777562355\n",
      "train loss:0.0069251330630098\n",
      "train loss:0.012904641484142755\n",
      "train loss:0.054613096233582056\n",
      "train loss:0.02689050580330159\n",
      "train loss:0.009902628993053412\n",
      "train loss:0.021664068288443854\n",
      "train loss:0.008083420440894287\n",
      "train loss:0.041619069652512115\n",
      "train loss:0.013416795271514468\n",
      "train loss:0.024738259470474828\n",
      "train loss:0.029958236792650205\n",
      "train loss:0.020348597329463883\n",
      "train loss:0.012292685303007114\n",
      "train loss:0.037173800606165444\n",
      "train loss:0.0077209947508284525\n",
      "train loss:0.01782716491149513\n",
      "train loss:0.026127922920664782\n",
      "train loss:0.018916385430674188\n",
      "train loss:0.0054037684242228415\n",
      "=== epoch:19, train acc:0.998, test acc:0.965 ===\n",
      "train loss:0.00593792420041667\n",
      "train loss:0.020721862492568535\n",
      "train loss:0.02692542400883713\n",
      "train loss:0.03758386027973512\n",
      "train loss:0.024568492043503566\n",
      "train loss:0.01848097793350991\n",
      "train loss:0.021243400468623087\n",
      "train loss:0.004708027196399992\n",
      "train loss:0.019885329122654668\n",
      "train loss:0.022953935879667705\n",
      "train loss:0.011306158719107083\n",
      "train loss:0.02744675161888978\n",
      "train loss:0.020126894447980027\n",
      "train loss:0.03195728337040688\n",
      "train loss:0.008038669967001347\n",
      "train loss:0.007545100270507024\n",
      "train loss:0.02378277038745891\n",
      "train loss:0.05547575040127108\n",
      "train loss:0.009826794114751755\n",
      "train loss:0.01865033239408456\n",
      "train loss:0.010458107434606479\n",
      "train loss:0.01943971658216619\n",
      "train loss:0.021846544620390585\n",
      "train loss:0.011635119695593282\n",
      "train loss:0.05852347506884462\n",
      "train loss:0.007432146322100972\n",
      "train loss:0.012198364956871637\n",
      "train loss:0.030930714025248253\n",
      "train loss:0.01063340161589446\n",
      "train loss:0.011574078716695168\n",
      "train loss:0.01788539939827975\n",
      "train loss:0.01593794659866156\n",
      "train loss:0.020711910984494732\n",
      "train loss:0.033880298824888494\n",
      "train loss:0.07293187963870511\n",
      "train loss:0.03113232890514693\n",
      "train loss:0.009803286189080718\n",
      "train loss:0.02425156867999337\n",
      "train loss:0.017924669941307248\n",
      "train loss:0.036712718161710726\n",
      "train loss:0.011847747025524274\n",
      "train loss:0.018147227331761707\n",
      "train loss:0.01942615012565167\n",
      "train loss:0.0232100801696387\n",
      "train loss:0.03418861536069486\n",
      "train loss:0.013919579545700285\n",
      "train loss:0.048079843868211285\n",
      "train loss:0.05438563975951448\n",
      "train loss:0.0159892359901866\n",
      "train loss:0.01199686912873667\n",
      "=== epoch:20, train acc:0.984, test acc:0.955 ===\n",
      "train loss:0.03295234716399106\n",
      "train loss:0.050282421959748504\n",
      "train loss:0.01747428217241293\n",
      "train loss:0.024219862686896385\n",
      "train loss:0.016932147011722225\n",
      "train loss:0.01892761232026568\n",
      "train loss:0.009683171450241949\n",
      "train loss:0.04469662633426608\n",
      "train loss:0.01777970141124842\n",
      "train loss:0.017948330385254216\n",
      "train loss:0.01657823042192863\n",
      "train loss:0.013101412168537864\n",
      "train loss:0.03360339710374174\n",
      "train loss:0.023870215663375896\n",
      "train loss:0.016098557057960997\n",
      "train loss:0.013856781630788151\n",
      "train loss:0.010139992591299354\n",
      "train loss:0.028949905057387152\n",
      "train loss:0.018684676892890516\n",
      "train loss:0.020018158634354008\n",
      "train loss:0.012145814274608414\n",
      "train loss:0.005542578449293728\n",
      "train loss:0.00897182861578919\n",
      "train loss:0.016080293259392885\n",
      "train loss:0.02625579491794376\n",
      "train loss:0.018749605735376196\n",
      "train loss:0.021571201303487345\n",
      "train loss:0.007889657258332542\n",
      "train loss:0.016385941996858222\n",
      "train loss:0.014161292325397297\n",
      "train loss:0.03986370177813897\n",
      "train loss:0.010478186573947825\n",
      "train loss:0.032962619792597285\n",
      "train loss:0.023209037869491977\n",
      "train loss:0.01870230528588539\n",
      "train loss:0.023769567058456502\n",
      "train loss:0.016413816931003532\n",
      "train loss:0.06883926450505248\n",
      "train loss:0.016895397954472414\n",
      "train loss:0.016269323813275196\n",
      "train loss:0.007131939176660445\n",
      "train loss:0.01582371915753432\n",
      "train loss:0.020178054761077396\n",
      "train loss:0.02669420287222056\n",
      "train loss:0.029505107096837273\n",
      "train loss:0.00918182494456486\n",
      "train loss:0.007060166601324197\n",
      "train loss:0.035881768209245574\n",
      "train loss:0.02953308391056197\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.962\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 处理花费时间较长的情况下减少数据 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
